# -*- coding: utf-8 -*-
"""BERT Embeddings and KNN Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FH-yr8xuW96_y1zNzHi3TypH3Pn0YJAo

# Step 1: Load the data
"""

import pandas as pd

df = pd.read_csv('steam_games.csv')

"""# Step 2: Initial data exploration"""

df.head()

missing_info = df.isnull().sum()
for col in df.columns:
    missing_count = missing_info[col]
    missing_pct = (missing_count / len(df)) * 100
    print(f"{col}: {missing_count} missing ({missing_pct:.1f}%)")

"""# Step 3: Analyze text data"""

df['desc_length'] = df['detailed_description'].fillna('').str.len()
df['short_desc_length'] = df['short_description'].fillna('').str.len()

print(f"Detailed description lengths - Min: {df['desc_length'].min()}, Max: {df['desc_length'].max()}, Mean: {df['desc_length'].mean():.0f}")
print(f"Short description lengths - Min: {df['short_desc_length'].min()}, Max: {df['short_desc_length'].max()}, Mean: {df['short_desc_length'].mean():.0f}")

# Check for HTML tags
html_pattern = r'<[^>]*>'
has_html = df['detailed_description'].fillna('').str.contains(html_pattern).sum()
print(f"\nGames with HTML tags: {has_html}")

"""# Step 4: Text cleaning function"""

import re

def clean_text(text):
    """Clean text for BERT processing"""
    if pd.isna(text) or text == '':
        return ''

    text = str(text)

    # Remove HTML tags
    text = re.sub(r'<[^>]*>', ' ', text)

    # Remove excessive whitespace and special characters
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s.,!?;:()\-\'"/]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()

    return text

"""# Step 5: Apply data cleaning"""

# Clean text columns
df['name_clean'] = df['name'].apply(clean_text)
df['detailed_description_clean'] = df['detailed_description'].apply(clean_text)
df['short_description_clean'] = df['short_description'].apply(clean_text)

# Handle missing detailed descriptions - use short description as fallback
mask = (df['detailed_description_clean'] == '') | df['detailed_description_clean'].isna()
df.loc[mask, 'detailed_description_clean'] = df.loc[mask, 'short_description_clean']

# Fill missing categorical data
df['genres'] = df['genres'].fillna('Unknown')
df['categories'] = df['categories'].fillna('Unknown')
df['developers'] = df['developers'].fillna('Unknown')
df['publishers'] = df['publishers'].fillna('Unknown')
df['price'] = df['price'].fillna(0)

"""# Step 6: Create combined text for BERT"""

df['combined_text'] = (
    df['name_clean'] + ' ' +
    df['detailed_description_clean'] + ' ' +
    df['genres'].apply(lambda x: x if x != 'Unknown' else '') + ' ' +
    df['categories'].apply(lambda x: x if x != 'Unknown' else '')
)

# Clean the combined text
df['combined_text'] = df['combined_text'].apply(clean_text)

# Filter out games with very short text (less than 50 characters)
original_count = len(df)
df = df[df['combined_text'].str.len() >= 50].copy()
filtered_count = len(df)

print(f"Original games: {original_count}")
print(f"After filtering: {filtered_count}")
print(f"Removed: {original_count - filtered_count} games with insufficient text")

"""# Step 7: Analyze cleaned data"""

text_lengths = df['combined_text'].str.len()
print(f"Combined text length - Min: {text_lengths.min()}, Max: {text_lengths.max()}, Mean: {text_lengths.mean():.0f}")

# Estimate token counts (roughly 1 token = 4 characters)
token_estimates = text_lengths / 4
print(f"Estimated tokens - Min: {token_estimates.min():.0f}, Max: {token_estimates.max():.0f}, Mean: {token_estimates.mean():.0f}")

# Check how many exceed BERT's 512 token limit
over_limit = (token_estimates > 512).sum()
print(f"Games over 512 tokens: {over_limit} ({(over_limit/len(df)*100):.1f}%)")

"""# Step 8: Show sample cleaned data"""

for i in range(3):
    game = df.iloc[i]
    print(f"\n{i+1}. {game['name']}")
    print(f"   Text length: {len(game['combined_text'])} chars")
    print(f"   Preview: {game['combined_text'][:200]}...")

"""# Step 9: Genre analysis"""

genre_counts = {}
for genres_str in df['genres']:
    if genres_str != 'Unknown' and pd.notna(genres_str):
        for genre in genres_str.split(','):
            genre = genre.strip()
            genre_counts[genre] = genre_counts.get(genre, 0) + 1

top_genres = sorted(genre_counts.items(), key=lambda x: x[1], reverse=True)[:10]
for genre, count in top_genres:
    print(f"{genre}: {count}")

"""# Step 10: Visualize data distribution"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.set()

# Text length distribution
plt.hist(text_lengths, bins=50, alpha=0.7, color='skyblue')
plt.title('Combined Text Length Distribution')
plt.xlabel('Characters')
plt.ylabel('Count')

plt.show()

# Price distribution
prices = df[df['price'] <= 100]['price']  # Filter extreme outliers for visualization
plt.hist(prices, bins=30, alpha=0.7, color='coral')
plt.title('Price Distribution (â‰¤$100)')
plt.xlabel('Price ($)')
plt.ylabel('Count')

plt.show()

# Top genres
genres, counts = zip(*top_genres[:8])
plt.barh(range(len(genres)), counts, color='lightpink')
plt.yticks(range(len(genres)), genres)
plt.title('Top 8 Genres')
plt.xlabel('Count')

plt.show()

df['release_year'] = pd.to_datetime(df['release_date'], errors='coerce').dt.year
years = df['release_year'].dropna()
# years = years[years >= 2000]  # Focus on recent games
plt.hist(years, bins=20, alpha=0.7, color='gold')
plt.title('Release Year Distribution')
plt.xlabel('Year')
plt.ylabel('Count')

plt.show()

"""# Step 11: Prepare final dataset for BERT"""

# Keep only necessary columns for the recommendation system
final_columns = [
    'app_id', 'name', 'combined_text', 'genres', 'categories',
    'price', 'release_date', 'developers', 'publishers'
]

df_final = df[final_columns].copy()

df_final.head()

"""# BERT Embeddings and KNN Recommendation System

# Step 1: Load cleaned data (assuming you have it from previous step)
# Step 2: Use full sentences (No truncation!)
"""

# Use the full combined text directly
df_final['bert_text'] = df_final['combined_text']

"""# Step 3: Initialize BERT model"""

from sentence_transformers import SentenceTransformer

# Using sentence-transformers - optimized for semantic similarity
model = SentenceTransformer('all-mpnet-base-v2')  # Fast and effective for recommendations

"""# Step 4: Generate embeddings in batches"""

from tqdm import tqdm
import numpy as np
import time

def generate_embeddings_batch(texts, model, batch_size=32):
    """Generate BERT embeddings in batches to manage memory"""
    embeddings = []

    print(f"Generating embeddings for {len(texts)} texts...")
    start_time = time.time()

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_embeddings = model.encode(batch, show_progress_bar=False)
        embeddings.extend(batch_embeddings)

        if (i // batch_size + 1) % 20 == 0:  # Progress update every 20 batches
            elapsed = time.time() - start_time
            progress = (i + batch_size) / len(texts) * 100
            print(f"Progress: {progress:.1f}% ({elapsed:.1f}s elapsed)")

    total_time = time.time() - start_time
    print(f"Embeddings generated in {total_time:.1f} seconds")

    return np.array(embeddings)

# Generate embeddings
embeddings = generate_embeddings_batch(df_final['bert_text'].tolist(), model, batch_size=32)

"""# Step 5: Build KNN model"""

from sklearn.neighbors import NearestNeighbors
from sklearn.metrics.pairwise import cosine_similarity

# Using cosine similarity (works well with BERT embeddings)
knn_model = NearestNeighbors(
    n_neighbors=10,  # Find 10 similar games
    metric='cosine',
    algorithm='brute'  # Most accurate for cosine similarity
)

knn_model.fit(embeddings)

"""# Step 6: Create recommendation function"""

def recommend_from_description(description, df, embeddings, knn_model, n_recommendations=5):
    """
    Recommend games based on a text description
    """
    # Use the full description without truncation
    clean_desc = description.strip()

    # Generate embedding for the description
    desc_embedding = model.encode([clean_desc])

    # Find similar games
    distances, indices = knn_model.kneighbors(desc_embedding, n_neighbors=n_recommendations)

    # Convert to similarity scores
    similarity_scores = 1 - distances[0]

    # Get recommended games
    recommendations = df.iloc[indices[0]].copy()
    recommendations['similarity_score'] = similarity_scores

    return recommendations[['name', 'genres', 'price', 'similarity_score']]

"""# Step 7: Test the recommendation system"""

# Test 2: Concrete semantic tests focusing on actual gameplay elements
# Short, concrete description focusing on actual gameplay elements
csgo_concrete = "5v5 rounds economy weapons tactical shooter defuse"

csgo_recs = recommend_from_description(
    csgo_concrete,
    df_final, embeddings, knn_model, n_recommendations=7
)

csgo_recs

# Test 3: Besiege concrete test
# Focus on concrete building and destruction elements
besiege_concrete = "build machines physics destruction cannons siege walls"

besiege_recs = recommend_from_description(
    besiege_concrete,
    df_final, embeddings, knn_model, n_recommendations=7
)

besiege_recs

# Test 4: Racing game test
racing_concrete = "cars racing tracks speed championship driving"

racing_recs = recommend_from_description(
    racing_concrete,
    df_final, embeddings, knn_model, n_recommendations=7
)

racing_recs

# Test 5: Puzzle game test
puzzle_concrete = "puzzles logic brain solve levels thinking"

puzzle_recs = recommend_from_description(
    puzzle_concrete,
    df_final, embeddings, knn_model, n_recommendations=7
)

puzzle_recs

"""# Step 8: Save the model and embeddings"""

# Download files directly to your computer
from google.colab import files

# Save and download
np.save('game_embeddings.npy', embeddings)
files.download('game_embeddings.npy')

import pickle

with open('knn_model.pkl', 'wb') as f:
    pickle.dump(knn_model, f)
files.download('knn_model.pkl')

# Save game data with indices
df_final.to_csv('games_with_embeddings.csv', index=False)
files.download('games_with_embeddings.csv')